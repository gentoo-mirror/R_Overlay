<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<longdescription>
		Local Language Model Inference // Enables R users to run large
		language models locally using 'GGUF' model files and the
		'llama.cpp' inference engine. Provides a complete R interface
		for loading models, generating text completions, and streaming
		responses in real-time. Supports local inference without
		requiring cloud APIs or internet connectivity, ensuring
		complete data privacy and control. References: 'Gerganov' et
		al. (2023) https://github.com/ggml-org/llama.cpp.
	</longdescription>
</pkgmetadata>
