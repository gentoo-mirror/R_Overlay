<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<longdescription>
		Gradient Descent for Regression Tasks // An implementation of
		various learning algorithms based on Gradient Descent for
		dealing with regression tasks. The variants of gradient descent
		algorithm are : Mini-Batch Gradient Descent (MBGD), which is an
		optimization to use training data partially to reduce the
		computation load. Stochastic Gradient Descent (SGD), which is
		an optimization to use a random data in learning to reduce the
		computation load drastically. Stochastic Average Gradient
		(SAG), which is a SGD-based algorithm to minimize stochastic
		step to average. Momentum Gradient Descent (MGD), which is an
		optimization to speed-up gradient descent learning. Accelerated
		Gradient Descent (AGD), which is an optimization to accelerate
		gradient descent learning. Adagrad, which is a gradient-
		descent-based algorithm that accumulate previous cost to do
		adaptive learning. Adadelta, which is a gradient-descent-based
		algorithm that use hessian approximation to do adaptive
		learning. RMSprop, which is a gradient-descent-based algorithm
		that combine Adagrad and Adadelta adaptive learning ability.
		Adam, which is a gradient-descent-based algorithm that mean and
		variance moment to do adaptive learning.
	</longdescription>
</pkgmetadata>
